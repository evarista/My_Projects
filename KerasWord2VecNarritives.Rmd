---
title: "Word2VecTutorial"
author: "Kim Evarista"
date: "02/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(readr)
setwd("~/Desktop/NLP/DataPracticum")
consumer <- read_csv("Consumer_Complaints.csv")
```
Data Processing and Cleaning 
```{r}
library(stringr)
consumer_new <- na.omit(consumer$`Consumer complaint narrative`)
consumer_new2 <- consumer_new[1:10000]

```

```{r}
library(stringr)
library(purrr)
consumer_new2 <- str_replace_all(consumer_new2,pattern ="[^[:alnum:]]|X{1,}", " ")
consumer_new2 <- str_to_lower(consumer_new2)
```

Text Preprocessing 
Tokenizer - segmenting text into words
```{r}
library(keras)
tokenizer <- text_tokenizer(num_words = 2000) #2000 words to keep 
tokenizer %>% fit_text_tokenizer(consumer_new2) #updates and fits the text vocabulary 
```

```{r}
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
  gen <- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip <- generator_next(gen) %>%
      skipgrams(
        vocabulary_size = tokenizer$num_words, 
        window_size = window_size, 
        negative_samples = 1
      )
    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
    y <- skip$labels %>% as.matrix(ncol = 1)
    list(x, y)
  }
}
#skipgrams_generator is a function, which generates skipgram word pairs. We need these word pairs to feed into the neural network to determine similar words (which will be used in a similar context).
```
Word Embedding - representation to score each word to determine how influential it is in representing corpora meaning. 
```{r}
embedding_size <- 128  # Dimension of the embedding vector.
skip_window <- 5       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
```

```{r}
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
```
The Embedding layer is the first layer in the neural network
```{r}
embedding <- layer_embedding(
  input_dim = tokenizer$num_words + 1, 
  output_dim = embedding_size, 
  input_length = 1, 
  name = "embedding"
)

target_vector <- input_target %>% 
  embedding() %>% 
  layer_flatten()

context_vector <- input_context %>%
  embedding() %>%
  layer_flatten()
```

```{r}
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
```

```{r}
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
```

```{r}
summary(model)
```

```{r}
model %>%
  fit_generator(
    skipgrams_generator(consumer_new, tokenizer, skip_window, negative_samples), 
    steps_per_epoch = 1000, epochs = 4
    )
```

```{r}
library(dplyr)

embedding_matrix <- get_weights(model)[[1]]

words <- data_frame(
  word = names(tokenizer$word_index), 
  id = as.integer(unlist(tokenizer$word_index))
)

words <- words %>%
  filter(id <= tokenizer$num_words) %>%
  arrange(id)

row.names(embedding_matrix) <- c("UNK", words$word)
```

```{r}
library(text2vec)

find_similar_words <- function(word, embedding_matrix, n = 5) {
  similarities <- embedding_matrix[word, , drop = FALSE] %>%
    sim2(embedding_matrix, y = ., method = "cosine")
  
  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)
}
```

```{r}
find_similar_words("very", embedding_matrix)
```

```{r}
find_similar_words("late", embedding_matrix)
```

```{r}
library(Rtsne)
library(ggplot2)
library(plotly)

tsne <- Rtsne(embedding_matrix[2:500,], perplexity = 50, pca = FALSE)

tsne_plot <- tsne$Y %>%
  as.data.frame() %>%
  mutate(word = row.names(embedding_matrix)[2:500]) %>%
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 3)
tsne_plot
```


------------------------------------------------------------------
For scars 
```{r}
length(words)
```


```{r}
testing_set <- list()
related <- data_frame()
input_word <- list() 
#x <- list()
for(i in 1:100){
#  x[i] <- find_similar_words(words$word[i], embedding_matrix) 
#  testing_set <- find_similar_words(words$word[i], embedding_matrix)
  related[i] <- as.vector(names(find_similar_words(words$word[i], embedding_matrix)))
  input_word[i] <- words$word[i]
}
```

```{r}
test <- list()
test <- find_similar_words(words$word[1], embedding_matrix)
test
str(test)
y <- names(test)
pastetest <- paste(y, )
pastetest
str(pastetest)
z <- y %>% data.frame()
z
try <- data_frame()
try[1] <- z
```

